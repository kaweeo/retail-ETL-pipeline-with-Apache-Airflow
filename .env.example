# Environment Variables - Example Configuration
# Copy this file to .env and fill in your actual values
# NEVER commit .env to version control (already in .gitignore)

# ============================================================================
# AWS CREDENTIALS
# ============================================================================
# Required for S3 access (extract and load operations)

export AWS_ACCESS_KEY_ID="your-aws-access-key-here"
export AWS_SECRET_ACCESS_KEY="your-aws-secret-key-here"

# Optional: AWS Session Token (for temporary credentials / IAM role assumption)
export AWS_SESSION_TOKEN="your-session-token-here"

# AWS Region for S3 operations
export AWS_DEFAULT_REGION="us-east-1"

# ============================================================================
# SNOWFLAKE CREDENTIALS
# ============================================================================
# Required for warehouse load operations and schema setup

export SNOWFLAKE_USER="your_snowflake_username"
export SNOWFLAKE_PASSWORD="your_snowflake_password"
export SNOWFLAKE_ACCOUNT="xy12345.us-east-1"  # Account ID without cloud provider suffix
export SNOWFLAKE_WAREHOUSE="COMPUTE_WH"
export SNOWFLAKE_DATABASE="RETAIL_DW"
export SNOWFLAKE_SCHEMA="RAW"
export SNOWFLAKE_ROLE="TRANSFORMER"

# Optional: Authentication method (can also use key-pair)
# export SNOWFLAKE_AUTHENTICATOR="externalbrowser"

# ============================================================================
# AIRFLOW CONFIGURATION
# ============================================================================
# Settings for Apache Airflow orchestration

# Airflow Home directory (default: ~/airflow)
export AIRFLOW_HOME="/home/user/airflow"

# Airflow Executor Type (local, celery, kubernetes)
export AIRFLOW_EXECUTOR="LocalExecutor"

# Airflow Database Backend (sqlite, postgres, mysql)
export AIRFLOW_DATABASE_URL="postgresql://user:password@localhost:5432/airflow"

# Optional: Email configuration for alerts
export AIRFLOW_SMTP_HOST="smtp.gmail.com"
export AIRFLOW_SMTP_PORT="587"
export AIRFLOW_SMTP_USER="your-email@gmail.com"
export AIRFLOW_SMTP_PASSWORD="your-gmail-app-password"
export AIRFLOW__CORE__UNIT_TEST_MODE="False"

# ============================================================================
# DOCKER & ASTRONOMER
# ============================================================================
# For containerized deployments with Astronomer

export ASTRONOMER_REGISTRY_URL="registry.astronomer.io"
export ASTRONOMER_API_KEY="your-astronomer-api-key"
export DOCKER_BUILDKIT="1"

# ============================================================================
# APPLICATION CONFIGURATION
# ============================================================================
# Custom settings for the Retail ETL Pipeline

# S3 Bucket Configuration
export S3_BUCKET="retail-data-warehouse"
export S3_RAW_FOLDER="retail-data/"
export S3_CLEANSED_FOLDER="cleansed-data/"

# Data Quality Thresholds
export MAX_DROP_RATE="0.10"  # Allow up to 10% row drops
export MIN_REVENUE_AMOUNT="0.01"  # Minimum revenue threshold

# Feature Flags
export ENABLE_SNOWFLAKE_LOAD="true"
export ENABLE_EMAIL_ALERTS="false"  # Set to true once SMTP configured
export ENABLE_DATA_PROFILING="false"  # Set to true for detailed quality reports

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log Level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
export LOG_LEVEL="INFO"

# Log Output Location
export LOG_FILE_PATH="/var/log/airflow/retail_etl.log"

# ============================================================================
# DEVELOPMENT/TESTING
# ============================================================================
# Only set for local development

export PYTHONPATH="/home/user/data_wh_etl:$PYTHONPATH"
export PYTEST_CACHE_DIR="/home/user/data_wh_etl/.pytest_cache"

# ============================================================================
# OPTIONAL: MONITORING & OBSERVABILITY
# ============================================================================

# Datadog (optional monitoring)
export DATADOG_API_KEY="your-datadog-api-key"
export DATADOG_APP_KEY="your-datadog-app-key"

# New Relic (optional APM)
export NEW_RELIC_API_KEY="your-new-relic-api-key"

# ============================================================================
# QUICK START
# ============================================================================
# 
# 1. Copy this file:
#    cp .env.example .env
#
# 2. Edit .env with your actual credentials:
#    nano .env
#
# 3. Load environment variables:
#    source .env
#
# 4. Verify configuration:
#    echo $AWS_ACCESS_KEY_ID          # Should print your key
#    echo $SNOWFLAKE_ACCOUNT          # Should print your account
#
# 5. Start Airflow:
#    astro dev start
#
# ============================================================================
# SECURITY NOTES
# ============================================================================
#
# ✓ NEVER commit .env to Git (it's in .gitignore)
# ✓ Use strong, unique passwords
# ✓ Rotate credentials regularly (every 90 days)
# ✓ Use IAM roles instead of static credentials when possible
# ✓ Store secrets in a vault (AWS Secrets Manager, HashiCorp Vault, etc.)
# ✓ For production, use environment-specific secret management:
#   - Astronomer Platform Secrets
#   - AWS Secrets Manager
#   - Snowflake Key Pair Authentication
#
# ============================================================================
