# Quick Start Guide

Fast reference for running the Retail ETL Data Warehouse project.

## âš¡ One-Time Setup

```bash
# 1. Navigate to project
cd data_wh_etl

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt
pip install pytest pytest-cov black flake8

# 4. Configure AWS (optional for local testing)
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret

# 5. Update config.yaml with your S3 bucket
nano include/config.yaml
```

## ğŸš€ Running Airflow Locally

```bash
# Start Airflow (runs in Docker)
astro dev start

# Open browser to http://localhost:8080 (admin/admin)

# Stop Airflow
astro dev stop

# View logs
astro dev logs -f
```

## ğŸ§ª Running Tests

```bash
# All tests
pytest tests/ -v

# Specific test file
pytest tests/test_validations.py -v
pytest tests/test_etl_functions.py -v

# With coverage report
pytest tests/ --cov=include/ --cov-report=term
pytest tests/ --cov=include/ --cov-report=html
# View: open htmlcov/index.html

# Only tests matching a pattern
pytest tests/ -k "revenue" -v
```

## ğŸ“ Code Quality

```bash
# Format code
black include/ dags/ tests/

# Check formatting
black --check include/ dags/ tests/

# Lint code
flake8 include/ dags/ tests/ --max-line-length=120

# Run all checks
black include/ dags/ tests/ && \
flake8 include/ dags/ tests/ --max-line-length=120 && \
pytest tests/ -v --cov=include/
```

## ğŸ“‚ Project Structure

```
data_wh_etl/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ retail_etl_dag.py          # Main DAG orchestration
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ config.yaml                # Configuration
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ extract_s3.py          # Extract data
â”‚   â”‚   â”œâ”€â”€ transform.py           # Transform logic
â”‚   â”‚   â””â”€â”€ load_s3_csv.py         # Load to S3
â”‚   â””â”€â”€ validations/
â”‚       â”œâ”€â”€ input_schemas.py       # Input validation rules
â”‚       â”œâ”€â”€ output_schemas.py      # Output validation rules
â”‚       â”œâ”€â”€ validate_inputs.py     # Validation functions
â”‚       â””â”€â”€ validate_outputs.py    # Final validation
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_validations.py        # Validation unit tests
â”‚   â”œâ”€â”€ test_etl_functions.py      # ETL unit tests
â”‚   â””â”€â”€ conftest.py                # Test configuration
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ setup_snowflake.txt        # Snowflake DDL
â”œâ”€â”€ README.md                       # Project overview
â”œâ”€â”€ ARCHITECTURE.md                 # System design
â”œâ”€â”€ CONTRIBUTING.md                 # Contributing guide
â””â”€â”€ pytest.ini                      # Pytest configuration
```

## ğŸ”‘ Key Files Explained

| File                    | Purpose                                                                        |
| ----------------------- | ------------------------------------------------------------------------------ |
| `retail_etl_dag.py`     | Defines the DAG with 5 tasks: extract â†’ validate â†’ transform â†’ validate â†’ load |
| `extract_s3.py`         | Reads sales.csv and product_data.json from S3                                  |
| `transform.py`          | Enriches data: merges products, calculates revenue, creates flags              |
| `input_schemas.py`      | Pandera schemas for raw data validation                                        |
| `output_schemas.py`     | Pandera schemas for clean data validation                                      |
| `test_validations.py`   | 25+ tests for validation functions                                             |
| `test_etl_functions.py` | 20+ tests for transform and ETL functions                                      |

## ğŸ› Debugging

```bash
# View DAG logs
astro dev logs -f

# Test DAG syntax
astro dev run dags test retail_etl_pipeline 2026-01-01

# Check available DAGs
astro dev run dags list

# Run single task
astro dev run tasks test retail_etl_pipeline extract_raw_data 2026-01-01
```

## ğŸ“Š Data Flow

```
RAW S3 â†’ EXTRACT â†’ VALIDATE INPUT â†’ TRANSFORM â†’ VALIDATE OUTPUT â†’ LOAD S3
(CSV)                               (enrich)       (quality check)
& JSON    â†“                          â†“                                â†“
         2500+ rows    â†’ clean rows â†’ calculate revenue      â†’ sales_clean.csv
                       â†’ merge products with sales
                       â†’ create business flags (is_discounted, is_in_stock)
                       â†’ extract date/hour dimensions
```

## ğŸ¯ Next Steps

1. **Run Tests**: `pytest tests/ -v` to verify everything works
2. **Read Architecture**: Check `ARCHITECTURE.md` for system design
3. **Set Up Locally**: Follow CONTRIBUTING.md for full setup
4. **Customize**: Update config.yaml for your AWS S3 bucket
5. **Deploy**: Use `astro dev start` to run Airflow locally

## ğŸ“– Additional Resources

-   [README.md](README.md) - Complete project overview
-   [ARCHITECTURE.md](ARCHITECTURE.md) - Technical architecture details
-   [CONTRIBUTING.md](CONTRIBUTING.md) - Dev setup and contribution guidelines
-   [include/config.yaml](include/config.yaml) - Configuration reference

## ğŸ’¡ Common Tasks

**Add a new validation rule:**

1. Update schema in `input_schemas.py` or `output_schemas.py`
2. Add test in `tests/test_validations.py`
3. Run: `pytest tests/test_validations.py -v`

**Add a new transformation:**

1. Add function in `include/etl/transform.py`
2. Add test in `tests/test_etl_functions.py`
3. Run: `pytest tests/test_etl_functions.py::TestTransformFunction::test_new_feature -v`

**Update documentation:**

1. Edit README.md (project overview)
2. Edit ARCHITECTURE.md (system design)
3. Edit function docstrings (code documentation)
